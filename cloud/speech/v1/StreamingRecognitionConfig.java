// Generated by the protocol buffer compiler.  DO NOT EDIT!
// source: google/speech/v1/cloud_speech.proto

package com.google.cloud.speech.v1;

/**
 * <pre>
 * Provides information to the recognizer that specifies how to process the
 * request.
 * </pre>
 *
 * Protobuf type {@code google.cloud.speech.v1.StreamingRecognitionConfig}
 */
public  final class StreamingRecognitionConfig extends
    com.google.protobuf.GeneratedMessageLite<
        StreamingRecognitionConfig, StreamingRecognitionConfig.Builder> implements
    // @@protoc_insertion_point(message_implements:google.cloud.speech.v1.StreamingRecognitionConfig)
    StreamingRecognitionConfigOrBuilder {
  private StreamingRecognitionConfig() {
  }
  public static final int CONFIG_FIELD_NUMBER = 1;
  private com.google.cloud.speech.v1.RecognitionConfig config_;
  /**
   * <pre>
   * *Required* Provides information to the recognizer that specifies how to
   * process the request.
   * </pre>
   *
   * <code>optional .google.cloud.speech.v1.RecognitionConfig config = 1;</code>
   */
  public boolean hasConfig() {
    return config_ != null;
  }
  /**
   * <pre>
   * *Required* Provides information to the recognizer that specifies how to
   * process the request.
   * </pre>
   *
   * <code>optional .google.cloud.speech.v1.RecognitionConfig config = 1;</code>
   */
  public com.google.cloud.speech.v1.RecognitionConfig getConfig() {
    return config_ == null ? com.google.cloud.speech.v1.RecognitionConfig.getDefaultInstance() : config_;
  }
  /**
   * <pre>
   * *Required* Provides information to the recognizer that specifies how to
   * process the request.
   * </pre>
   *
   * <code>optional .google.cloud.speech.v1.RecognitionConfig config = 1;</code>
   */
  private void setConfig(com.google.cloud.speech.v1.RecognitionConfig value) {
    if (value == null) {
      throw new NullPointerException();
    }
    config_ = value;
    
    }
  /**
   * <pre>
   * *Required* Provides information to the recognizer that specifies how to
   * process the request.
   * </pre>
   *
   * <code>optional .google.cloud.speech.v1.RecognitionConfig config = 1;</code>
   */
  private void setConfig(
      com.google.cloud.speech.v1.RecognitionConfig.Builder builderForValue) {
    config_ = builderForValue.build();
    
  }
  /**
   * <pre>
   * *Required* Provides information to the recognizer that specifies how to
   * process the request.
   * </pre>
   *
   * <code>optional .google.cloud.speech.v1.RecognitionConfig config = 1;</code>
   */
  private void mergeConfig(com.google.cloud.speech.v1.RecognitionConfig value) {
    if (config_ != null &&
        config_ != com.google.cloud.speech.v1.RecognitionConfig.getDefaultInstance()) {
      config_ =
        com.google.cloud.speech.v1.RecognitionConfig.newBuilder(config_).mergeFrom(value).buildPartial();
    } else {
      config_ = value;
    }
    
  }
  /**
   * <pre>
   * *Required* Provides information to the recognizer that specifies how to
   * process the request.
   * </pre>
   *
   * <code>optional .google.cloud.speech.v1.RecognitionConfig config = 1;</code>
   */
  private void clearConfig() {  config_ = null;
    
  }

  public static final int SINGLE_UTTERANCE_FIELD_NUMBER = 2;
  private boolean singleUtterance_;
  /**
   * <pre>
   * *Optional* If `false` or omitted, the recognizer will perform continuous
   * recognition (continuing to wait for and process audio even if the user
   * pauses speaking) until the client closes the input stream (gRPC API) or
   * until the maximum time limit has been reached. May return multiple
   * `StreamingRecognitionResult`s with the `is_final` flag set to `true`.
   * If `true`, the recognizer will detect a single spoken utterance. When it
   * detects that the user has paused or stopped speaking, it will return an
   * `END_OF_SINGLE_UTTERANCE` event and cease recognition. It will return no
   * more than one `StreamingRecognitionResult` with the `is_final` flag set to
   * `true`.
   * </pre>
   *
   * <code>optional bool single_utterance = 2;</code>
   */
  public boolean getSingleUtterance() {
    return singleUtterance_;
  }
  /**
   * <pre>
   * *Optional* If `false` or omitted, the recognizer will perform continuous
   * recognition (continuing to wait for and process audio even if the user
   * pauses speaking) until the client closes the input stream (gRPC API) or
   * until the maximum time limit has been reached. May return multiple
   * `StreamingRecognitionResult`s with the `is_final` flag set to `true`.
   * If `true`, the recognizer will detect a single spoken utterance. When it
   * detects that the user has paused or stopped speaking, it will return an
   * `END_OF_SINGLE_UTTERANCE` event and cease recognition. It will return no
   * more than one `StreamingRecognitionResult` with the `is_final` flag set to
   * `true`.
   * </pre>
   *
   * <code>optional bool single_utterance = 2;</code>
   */
  private void setSingleUtterance(boolean value) {
    
    singleUtterance_ = value;
  }
  /**
   * <pre>
   * *Optional* If `false` or omitted, the recognizer will perform continuous
   * recognition (continuing to wait for and process audio even if the user
   * pauses speaking) until the client closes the input stream (gRPC API) or
   * until the maximum time limit has been reached. May return multiple
   * `StreamingRecognitionResult`s with the `is_final` flag set to `true`.
   * If `true`, the recognizer will detect a single spoken utterance. When it
   * detects that the user has paused or stopped speaking, it will return an
   * `END_OF_SINGLE_UTTERANCE` event and cease recognition. It will return no
   * more than one `StreamingRecognitionResult` with the `is_final` flag set to
   * `true`.
   * </pre>
   *
   * <code>optional bool single_utterance = 2;</code>
   */
  private void clearSingleUtterance() {
    
    singleUtterance_ = false;
  }

  public static final int INTERIM_RESULTS_FIELD_NUMBER = 3;
  private boolean interimResults_;
  /**
   * <pre>
   * *Optional* If `true`, interim results (tentative hypotheses) may be
   * returned as they become available (these interim results are indicated with
   * the `is_final=false` flag).
   * If `false` or omitted, only `is_final=true` result(s) are returned.
   * </pre>
   *
   * <code>optional bool interim_results = 3;</code>
   */
  public boolean getInterimResults() {
    return interimResults_;
  }
  /**
   * <pre>
   * *Optional* If `true`, interim results (tentative hypotheses) may be
   * returned as they become available (these interim results are indicated with
   * the `is_final=false` flag).
   * If `false` or omitted, only `is_final=true` result(s) are returned.
   * </pre>
   *
   * <code>optional bool interim_results = 3;</code>
   */
  private void setInterimResults(boolean value) {
    
    interimResults_ = value;
  }
  /**
   * <pre>
   * *Optional* If `true`, interim results (tentative hypotheses) may be
   * returned as they become available (these interim results are indicated with
   * the `is_final=false` flag).
   * If `false` or omitted, only `is_final=true` result(s) are returned.
   * </pre>
   *
   * <code>optional bool interim_results = 3;</code>
   */
  private void clearInterimResults() {
    
    interimResults_ = false;
  }

  public void writeTo(com.google.protobuf.CodedOutputStream output)
                      throws java.io.IOException {
    if (config_ != null) {
      output.writeMessage(1, getConfig());
    }
    if (singleUtterance_ != false) {
      output.writeBool(2, singleUtterance_);
    }
    if (interimResults_ != false) {
      output.writeBool(3, interimResults_);
    }
  }

  public int getSerializedSize() {
    int size = memoizedSerializedSize;
    if (size != -1) return size;

    size = 0;
    if (config_ != null) {
      size += com.google.protobuf.CodedOutputStream
        .computeMessageSize(1, getConfig());
    }
    if (singleUtterance_ != false) {
      size += com.google.protobuf.CodedOutputStream
        .computeBoolSize(2, singleUtterance_);
    }
    if (interimResults_ != false) {
      size += com.google.protobuf.CodedOutputStream
        .computeBoolSize(3, interimResults_);
    }
    memoizedSerializedSize = size;
    return size;
  }

  public static com.google.cloud.speech.v1.StreamingRecognitionConfig parseFrom(
      com.google.protobuf.ByteString data)
      throws com.google.protobuf.InvalidProtocolBufferException {
    return com.google.protobuf.GeneratedMessageLite.parseFrom(
        DEFAULT_INSTANCE, data);
  }
  public static com.google.cloud.speech.v1.StreamingRecognitionConfig parseFrom(
      com.google.protobuf.ByteString data,
      com.google.protobuf.ExtensionRegistryLite extensionRegistry)
      throws com.google.protobuf.InvalidProtocolBufferException {
    return com.google.protobuf.GeneratedMessageLite.parseFrom(
        DEFAULT_INSTANCE, data, extensionRegistry);
  }
  public static com.google.cloud.speech.v1.StreamingRecognitionConfig parseFrom(byte[] data)
      throws com.google.protobuf.InvalidProtocolBufferException {
    return com.google.protobuf.GeneratedMessageLite.parseFrom(
        DEFAULT_INSTANCE, data);
  }
  public static com.google.cloud.speech.v1.StreamingRecognitionConfig parseFrom(
      byte[] data,
      com.google.protobuf.ExtensionRegistryLite extensionRegistry)
      throws com.google.protobuf.InvalidProtocolBufferException {
    return com.google.protobuf.GeneratedMessageLite.parseFrom(
        DEFAULT_INSTANCE, data, extensionRegistry);
  }
  public static com.google.cloud.speech.v1.StreamingRecognitionConfig parseFrom(java.io.InputStream input)
      throws java.io.IOException {
    return com.google.protobuf.GeneratedMessageLite.parseFrom(
        DEFAULT_INSTANCE, input);
  }
  public static com.google.cloud.speech.v1.StreamingRecognitionConfig parseFrom(
      java.io.InputStream input,
      com.google.protobuf.ExtensionRegistryLite extensionRegistry)
      throws java.io.IOException {
    return com.google.protobuf.GeneratedMessageLite.parseFrom(
        DEFAULT_INSTANCE, input, extensionRegistry);
  }
  public static com.google.cloud.speech.v1.StreamingRecognitionConfig parseDelimitedFrom(java.io.InputStream input)
      throws java.io.IOException {
    return parseDelimitedFrom(DEFAULT_INSTANCE, input);
  }
  public static com.google.cloud.speech.v1.StreamingRecognitionConfig parseDelimitedFrom(
      java.io.InputStream input,
      com.google.protobuf.ExtensionRegistryLite extensionRegistry)
      throws java.io.IOException {
    return parseDelimitedFrom(DEFAULT_INSTANCE, input, extensionRegistry);
  }
  public static com.google.cloud.speech.v1.StreamingRecognitionConfig parseFrom(
      com.google.protobuf.CodedInputStream input)
      throws java.io.IOException {
    return com.google.protobuf.GeneratedMessageLite.parseFrom(
        DEFAULT_INSTANCE, input);
  }
  public static com.google.cloud.speech.v1.StreamingRecognitionConfig parseFrom(
      com.google.protobuf.CodedInputStream input,
      com.google.protobuf.ExtensionRegistryLite extensionRegistry)
      throws java.io.IOException {
    return com.google.protobuf.GeneratedMessageLite.parseFrom(
        DEFAULT_INSTANCE, input, extensionRegistry);
  }

  public static Builder newBuilder() {
    return DEFAULT_INSTANCE.toBuilder();
  }
  public static Builder newBuilder(com.google.cloud.speech.v1.StreamingRecognitionConfig prototype) {
    return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
  }

  /**
   * <pre>
   * Provides information to the recognizer that specifies how to process the
   * request.
   * </pre>
   *
   * Protobuf type {@code google.cloud.speech.v1.StreamingRecognitionConfig}
   */
  public static final class Builder extends
      com.google.protobuf.GeneratedMessageLite.Builder<
        com.google.cloud.speech.v1.StreamingRecognitionConfig, Builder> implements
      // @@protoc_insertion_point(builder_implements:google.cloud.speech.v1.StreamingRecognitionConfig)
      com.google.cloud.speech.v1.StreamingRecognitionConfigOrBuilder {
    // Construct using com.google.cloud.speech.v1.StreamingRecognitionConfig.newBuilder()
    private Builder() {
      super(DEFAULT_INSTANCE);
    }


    /**
     * <pre>
     * *Required* Provides information to the recognizer that specifies how to
     * process the request.
     * </pre>
     *
     * <code>optional .google.cloud.speech.v1.RecognitionConfig config = 1;</code>
     */
    public boolean hasConfig() {
      return instance.hasConfig();
    }
    /**
     * <pre>
     * *Required* Provides information to the recognizer that specifies how to
     * process the request.
     * </pre>
     *
     * <code>optional .google.cloud.speech.v1.RecognitionConfig config = 1;</code>
     */
    public com.google.cloud.speech.v1.RecognitionConfig getConfig() {
      return instance.getConfig();
    }
    /**
     * <pre>
     * *Required* Provides information to the recognizer that specifies how to
     * process the request.
     * </pre>
     *
     * <code>optional .google.cloud.speech.v1.RecognitionConfig config = 1;</code>
     */
    public Builder setConfig(com.google.cloud.speech.v1.RecognitionConfig value) {
      copyOnWrite();
      instance.setConfig(value);
      return this;
      }
    /**
     * <pre>
     * *Required* Provides information to the recognizer that specifies how to
     * process the request.
     * </pre>
     *
     * <code>optional .google.cloud.speech.v1.RecognitionConfig config = 1;</code>
     */
    public Builder setConfig(
        com.google.cloud.speech.v1.RecognitionConfig.Builder builderForValue) {
      copyOnWrite();
      instance.setConfig(builderForValue);
      return this;
    }
    /**
     * <pre>
     * *Required* Provides information to the recognizer that specifies how to
     * process the request.
     * </pre>
     *
     * <code>optional .google.cloud.speech.v1.RecognitionConfig config = 1;</code>
     */
    public Builder mergeConfig(com.google.cloud.speech.v1.RecognitionConfig value) {
      copyOnWrite();
      instance.mergeConfig(value);
      return this;
    }
    /**
     * <pre>
     * *Required* Provides information to the recognizer that specifies how to
     * process the request.
     * </pre>
     *
     * <code>optional .google.cloud.speech.v1.RecognitionConfig config = 1;</code>
     */
    public Builder clearConfig() {  copyOnWrite();
      instance.clearConfig();
      return this;
    }

    /**
     * <pre>
     * *Optional* If `false` or omitted, the recognizer will perform continuous
     * recognition (continuing to wait for and process audio even if the user
     * pauses speaking) until the client closes the input stream (gRPC API) or
     * until the maximum time limit has been reached. May return multiple
     * `StreamingRecognitionResult`s with the `is_final` flag set to `true`.
     * If `true`, the recognizer will detect a single spoken utterance. When it
     * detects that the user has paused or stopped speaking, it will return an
     * `END_OF_SINGLE_UTTERANCE` event and cease recognition. It will return no
     * more than one `StreamingRecognitionResult` with the `is_final` flag set to
     * `true`.
     * </pre>
     *
     * <code>optional bool single_utterance = 2;</code>
     */
    public boolean getSingleUtterance() {
      return instance.getSingleUtterance();
    }
    /**
     * <pre>
     * *Optional* If `false` or omitted, the recognizer will perform continuous
     * recognition (continuing to wait for and process audio even if the user
     * pauses speaking) until the client closes the input stream (gRPC API) or
     * until the maximum time limit has been reached. May return multiple
     * `StreamingRecognitionResult`s with the `is_final` flag set to `true`.
     * If `true`, the recognizer will detect a single spoken utterance. When it
     * detects that the user has paused or stopped speaking, it will return an
     * `END_OF_SINGLE_UTTERANCE` event and cease recognition. It will return no
     * more than one `StreamingRecognitionResult` with the `is_final` flag set to
     * `true`.
     * </pre>
     *
     * <code>optional bool single_utterance = 2;</code>
     */
    public Builder setSingleUtterance(boolean value) {
      copyOnWrite();
      instance.setSingleUtterance(value);
      return this;
    }
    /**
     * <pre>
     * *Optional* If `false` or omitted, the recognizer will perform continuous
     * recognition (continuing to wait for and process audio even if the user
     * pauses speaking) until the client closes the input stream (gRPC API) or
     * until the maximum time limit has been reached. May return multiple
     * `StreamingRecognitionResult`s with the `is_final` flag set to `true`.
     * If `true`, the recognizer will detect a single spoken utterance. When it
     * detects that the user has paused or stopped speaking, it will return an
     * `END_OF_SINGLE_UTTERANCE` event and cease recognition. It will return no
     * more than one `StreamingRecognitionResult` with the `is_final` flag set to
     * `true`.
     * </pre>
     *
     * <code>optional bool single_utterance = 2;</code>
     */
    public Builder clearSingleUtterance() {
      copyOnWrite();
      instance.clearSingleUtterance();
      return this;
    }

    /**
     * <pre>
     * *Optional* If `true`, interim results (tentative hypotheses) may be
     * returned as they become available (these interim results are indicated with
     * the `is_final=false` flag).
     * If `false` or omitted, only `is_final=true` result(s) are returned.
     * </pre>
     *
     * <code>optional bool interim_results = 3;</code>
     */
    public boolean getInterimResults() {
      return instance.getInterimResults();
    }
    /**
     * <pre>
     * *Optional* If `true`, interim results (tentative hypotheses) may be
     * returned as they become available (these interim results are indicated with
     * the `is_final=false` flag).
     * If `false` or omitted, only `is_final=true` result(s) are returned.
     * </pre>
     *
     * <code>optional bool interim_results = 3;</code>
     */
    public Builder setInterimResults(boolean value) {
      copyOnWrite();
      instance.setInterimResults(value);
      return this;
    }
    /**
     * <pre>
     * *Optional* If `true`, interim results (tentative hypotheses) may be
     * returned as they become available (these interim results are indicated with
     * the `is_final=false` flag).
     * If `false` or omitted, only `is_final=true` result(s) are returned.
     * </pre>
     *
     * <code>optional bool interim_results = 3;</code>
     */
    public Builder clearInterimResults() {
      copyOnWrite();
      instance.clearInterimResults();
      return this;
    }

    // @@protoc_insertion_point(builder_scope:google.cloud.speech.v1.StreamingRecognitionConfig)
  }
  protected final Object dynamicMethod(
      com.google.protobuf.GeneratedMessageLite.MethodToInvoke method,
      Object arg0, Object arg1) {
    switch (method) {
      case NEW_MUTABLE_INSTANCE: {
        return new com.google.cloud.speech.v1.StreamingRecognitionConfig();
      }
      case IS_INITIALIZED: {
        return DEFAULT_INSTANCE;
      }
      case MAKE_IMMUTABLE: {
        return null;
      }
      case NEW_BUILDER: {
        return new Builder();
      }
      case VISIT: {
        Visitor visitor = (Visitor) arg0;
        com.google.cloud.speech.v1.StreamingRecognitionConfig other = (com.google.cloud.speech.v1.StreamingRecognitionConfig) arg1;
        config_ = visitor.visitMessage(config_, other.config_);
        singleUtterance_ = visitor.visitBoolean(singleUtterance_ != false, singleUtterance_,
            other.singleUtterance_ != false, other.singleUtterance_);
        interimResults_ = visitor.visitBoolean(interimResults_ != false, interimResults_,
            other.interimResults_ != false, other.interimResults_);
        if (visitor == com.google.protobuf.GeneratedMessageLite.MergeFromVisitor
            .INSTANCE) {
        }
        return this;
      }
      case MERGE_FROM_STREAM: {
        com.google.protobuf.CodedInputStream input =
            (com.google.protobuf.CodedInputStream) arg0;
        com.google.protobuf.ExtensionRegistryLite extensionRegistry =
            (com.google.protobuf.ExtensionRegistryLite) arg1;
        try {
          boolean done = false;
          while (!done) {
            int tag = input.readTag();
            switch (tag) {
              case 0:
                done = true;
                break;
              default: {
                if (!input.skipField(tag)) {
                  done = true;
                }
                break;
              }
              case 10: {
                com.google.cloud.speech.v1.RecognitionConfig.Builder subBuilder = null;
                if (config_ != null) {
                  subBuilder = config_.toBuilder();
                }
                config_ = input.readMessage(com.google.cloud.speech.v1.RecognitionConfig.parser(), extensionRegistry);
                if (subBuilder != null) {
                  subBuilder.mergeFrom(config_);
                  config_ = subBuilder.buildPartial();
                }

                break;
              }
              case 16: {

                singleUtterance_ = input.readBool();
                break;
              }
              case 24: {

                interimResults_ = input.readBool();
                break;
              }
            }
          }
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          throw new RuntimeException(e.setUnfinishedMessage(this));
        } catch (java.io.IOException e) {
          throw new RuntimeException(
              new com.google.protobuf.InvalidProtocolBufferException(
                  e.getMessage()).setUnfinishedMessage(this));
        } finally {
        }
      }
      case GET_DEFAULT_INSTANCE: {
        return DEFAULT_INSTANCE;
      }
      case GET_PARSER: {
        if (PARSER == null) {    synchronized (com.google.cloud.speech.v1.StreamingRecognitionConfig.class) {
            if (PARSER == null) {
              PARSER = new DefaultInstanceBasedParser(DEFAULT_INSTANCE);
            }
          }
        }
        return PARSER;
      }
    }
    throw new UnsupportedOperationException();
  }


  // @@protoc_insertion_point(class_scope:google.cloud.speech.v1.StreamingRecognitionConfig)
  private static final com.google.cloud.speech.v1.StreamingRecognitionConfig DEFAULT_INSTANCE;
  static {
    DEFAULT_INSTANCE = new StreamingRecognitionConfig();
    DEFAULT_INSTANCE.makeImmutable();
  }

  public static com.google.cloud.speech.v1.StreamingRecognitionConfig getDefaultInstance() {
    return DEFAULT_INSTANCE;
  }

  private static volatile com.google.protobuf.Parser<StreamingRecognitionConfig> PARSER;

  public static com.google.protobuf.Parser<StreamingRecognitionConfig> parser() {
    return DEFAULT_INSTANCE.getParserForType();
  }
}

