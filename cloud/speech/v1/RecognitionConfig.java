// Generated by the protocol buffer compiler.  DO NOT EDIT!
// source: google/speech/v1/cloud_speech.proto

package com.google.cloud.speech.v1;

/**
 * <pre>
 * Provides information to the recognizer that specifies how to process the
 * request.
 * </pre>
 *
 * Protobuf type {@code google.cloud.speech.v1.RecognitionConfig}
 */
public  final class RecognitionConfig extends
    com.google.protobuf.GeneratedMessageLite<
        RecognitionConfig, RecognitionConfig.Builder> implements
    // @@protoc_insertion_point(message_implements:google.cloud.speech.v1.RecognitionConfig)
    RecognitionConfigOrBuilder {
  private RecognitionConfig() {
    languageCode_ = "";
    speechContexts_ = emptyProtobufList();
  }
  /**
   * <pre>
   * Audio encoding of the data sent in the audio message. All encodings support
   * only 1 channel (mono) audio. Only `FLAC` and `WAV` include a header that
   * describes the bytes of audio that follow the header. The other encodings
   * are raw audio bytes with no header.
   * For best results, the audio source should be captured and transmitted using
   * a lossless encoding (`FLAC` or `LINEAR16`). Recognition accuracy may be
   * reduced if lossy codecs, which include the other codecs listed in
   * this section, are used to capture or transmit the audio, particularly if
   * background noise is present.
   * </pre>
   *
   * Protobuf enum {@code google.cloud.speech.v1.RecognitionConfig.AudioEncoding}
   */
  public enum AudioEncoding
      implements com.google.protobuf.Internal.EnumLite {
    /**
     * <pre>
     * Not specified. Will return result [google.rpc.Code.INVALID_ARGUMENT][google.rpc.Code.INVALID_ARGUMENT].
     * </pre>
     *
     * <code>ENCODING_UNSPECIFIED = 0;</code>
     */
    ENCODING_UNSPECIFIED(0),
    /**
     * <pre>
     * Uncompressed 16-bit signed little-endian samples (Linear PCM).
     * </pre>
     *
     * <code>LINEAR16 = 1;</code>
     */
    LINEAR16(1),
    /**
     * <pre>
     * [`FLAC`](https://xiph.org/flac/documentation.html) (Free Lossless Audio
     * Codec) is the recommended encoding because it is
     * lossless--therefore recognition is not compromised--and
     * requires only about half the bandwidth of `LINEAR16`. `FLAC` stream
     * encoding supports 16-bit and 24-bit samples, however, not all fields in
     * `STREAMINFO` are supported.
     * </pre>
     *
     * <code>FLAC = 2;</code>
     */
    FLAC(2),
    /**
     * <pre>
     * 8-bit samples that compand 14-bit audio samples using G.711 PCMU/mu-law.
     * </pre>
     *
     * <code>MULAW = 3;</code>
     */
    MULAW(3),
    /**
     * <pre>
     * Adaptive Multi-Rate Narrowband codec. `sample_rate_hertz` must be 8000.
     * </pre>
     *
     * <code>AMR = 4;</code>
     */
    AMR(4),
    /**
     * <pre>
     * Adaptive Multi-Rate Wideband codec. `sample_rate_hertz` must be 16000.
     * </pre>
     *
     * <code>AMR_WB = 5;</code>
     */
    AMR_WB(5),
    /**
     * <pre>
     * Opus encoded audio frames in Ogg container
     * ([OggOpus](https://wiki.xiph.org/OggOpus)).
     * `sample_rate_hertz` must be 16000.
     * </pre>
     *
     * <code>OGG_OPUS = 6;</code>
     */
    OGG_OPUS(6),
    /**
     * <pre>
     * Although the use of lossy encodings is not recommended, if a very low
     * bitrate encoding is required, `OGG_OPUS` is highly preferred over
     * Speex encoding. The [Speex](https://speex.org/)  encoding supported by
     * Cloud Speech API has a header byte in each block, as in MIME type
     * `audio/x-speex-with-header-byte`.
     * It is a variant of the RTP Speex encoding defined in
     * [RFC 5574](https://tools.ietf.org/html/rfc5574).
     * The stream is a sequence of blocks, one block per RTP packet. Each block
     * starts with a byte containing the length of the block, in bytes, followed
     * by one or more frames of Speex data, padded to an integral number of
     * bytes (octets) as specified in RFC 5574. In other words, each RTP header
     * is replaced with a single byte containing the block length. Only Speex
     * wideband is supported. `sample_rate_hertz` must be 16000.
     * </pre>
     *
     * <code>SPEEX_WITH_HEADER_BYTE = 7;</code>
     */
    SPEEX_WITH_HEADER_BYTE(7),
    UNRECOGNIZED(-1),
    ;

    /**
     * <pre>
     * Not specified. Will return result [google.rpc.Code.INVALID_ARGUMENT][google.rpc.Code.INVALID_ARGUMENT].
     * </pre>
     *
     * <code>ENCODING_UNSPECIFIED = 0;</code>
     */
    public static final int ENCODING_UNSPECIFIED_VALUE = 0;
    /**
     * <pre>
     * Uncompressed 16-bit signed little-endian samples (Linear PCM).
     * </pre>
     *
     * <code>LINEAR16 = 1;</code>
     */
    public static final int LINEAR16_VALUE = 1;
    /**
     * <pre>
     * [`FLAC`](https://xiph.org/flac/documentation.html) (Free Lossless Audio
     * Codec) is the recommended encoding because it is
     * lossless--therefore recognition is not compromised--and
     * requires only about half the bandwidth of `LINEAR16`. `FLAC` stream
     * encoding supports 16-bit and 24-bit samples, however, not all fields in
     * `STREAMINFO` are supported.
     * </pre>
     *
     * <code>FLAC = 2;</code>
     */
    public static final int FLAC_VALUE = 2;
    /**
     * <pre>
     * 8-bit samples that compand 14-bit audio samples using G.711 PCMU/mu-law.
     * </pre>
     *
     * <code>MULAW = 3;</code>
     */
    public static final int MULAW_VALUE = 3;
    /**
     * <pre>
     * Adaptive Multi-Rate Narrowband codec. `sample_rate_hertz` must be 8000.
     * </pre>
     *
     * <code>AMR = 4;</code>
     */
    public static final int AMR_VALUE = 4;
    /**
     * <pre>
     * Adaptive Multi-Rate Wideband codec. `sample_rate_hertz` must be 16000.
     * </pre>
     *
     * <code>AMR_WB = 5;</code>
     */
    public static final int AMR_WB_VALUE = 5;
    /**
     * <pre>
     * Opus encoded audio frames in Ogg container
     * ([OggOpus](https://wiki.xiph.org/OggOpus)).
     * `sample_rate_hertz` must be 16000.
     * </pre>
     *
     * <code>OGG_OPUS = 6;</code>
     */
    public static final int OGG_OPUS_VALUE = 6;
    /**
     * <pre>
     * Although the use of lossy encodings is not recommended, if a very low
     * bitrate encoding is required, `OGG_OPUS` is highly preferred over
     * Speex encoding. The [Speex](https://speex.org/)  encoding supported by
     * Cloud Speech API has a header byte in each block, as in MIME type
     * `audio/x-speex-with-header-byte`.
     * It is a variant of the RTP Speex encoding defined in
     * [RFC 5574](https://tools.ietf.org/html/rfc5574).
     * The stream is a sequence of blocks, one block per RTP packet. Each block
     * starts with a byte containing the length of the block, in bytes, followed
     * by one or more frames of Speex data, padded to an integral number of
     * bytes (octets) as specified in RFC 5574. In other words, each RTP header
     * is replaced with a single byte containing the block length. Only Speex
     * wideband is supported. `sample_rate_hertz` must be 16000.
     * </pre>
     *
     * <code>SPEEX_WITH_HEADER_BYTE = 7;</code>
     */
    public static final int SPEEX_WITH_HEADER_BYTE_VALUE = 7;


    public final int getNumber() {
      return value;
    }

    /**
     * @deprecated Use {@link #forNumber(int)} instead.
     */
    @java.lang.Deprecated
    public static AudioEncoding valueOf(int value) {
      return forNumber(value);
    }

    public static AudioEncoding forNumber(int value) {
      switch (value) {
        case 0: return ENCODING_UNSPECIFIED;
        case 1: return LINEAR16;
        case 2: return FLAC;
        case 3: return MULAW;
        case 4: return AMR;
        case 5: return AMR_WB;
        case 6: return OGG_OPUS;
        case 7: return SPEEX_WITH_HEADER_BYTE;
        default: return null;
      }
    }

    public static com.google.protobuf.Internal.EnumLiteMap<AudioEncoding>
        internalGetValueMap() {
      return internalValueMap;
    }
    private static final com.google.protobuf.Internal.EnumLiteMap<
        AudioEncoding> internalValueMap =
          new com.google.protobuf.Internal.EnumLiteMap<AudioEncoding>() {
            public AudioEncoding findValueByNumber(int number) {
              return AudioEncoding.forNumber(number);
            }
          };

    private final int value;

    private AudioEncoding(int value) {
      this.value = value;
    }

    // @@protoc_insertion_point(enum_scope:google.cloud.speech.v1.RecognitionConfig.AudioEncoding)
  }

  private int bitField0_;
  public static final int ENCODING_FIELD_NUMBER = 1;
  private int encoding_;
  /**
   * <pre>
   * *Required* Encoding of audio data sent in all `RecognitionAudio` messages.
   * </pre>
   *
   * <code>optional .google.cloud.speech.v1.RecognitionConfig.AudioEncoding encoding = 1;</code>
   */
  public int getEncodingValue() {
    return encoding_;
  }
  /**
   * <pre>
   * *Required* Encoding of audio data sent in all `RecognitionAudio` messages.
   * </pre>
   *
   * <code>optional .google.cloud.speech.v1.RecognitionConfig.AudioEncoding encoding = 1;</code>
   */
  public com.google.cloud.speech.v1.RecognitionConfig.AudioEncoding getEncoding() {
    com.google.cloud.speech.v1.RecognitionConfig.AudioEncoding result = com.google.cloud.speech.v1.RecognitionConfig.AudioEncoding.forNumber(encoding_);
    return result == null ? com.google.cloud.speech.v1.RecognitionConfig.AudioEncoding.UNRECOGNIZED : result;
  }
  /**
   * <pre>
   * *Required* Encoding of audio data sent in all `RecognitionAudio` messages.
   * </pre>
   *
   * <code>optional .google.cloud.speech.v1.RecognitionConfig.AudioEncoding encoding = 1;</code>
   */
  private void setEncodingValue(int value) {
      encoding_ = value;
  }
  /**
   * <pre>
   * *Required* Encoding of audio data sent in all `RecognitionAudio` messages.
   * </pre>
   *
   * <code>optional .google.cloud.speech.v1.RecognitionConfig.AudioEncoding encoding = 1;</code>
   */
  private void setEncoding(com.google.cloud.speech.v1.RecognitionConfig.AudioEncoding value) {
    if (value == null) {
      throw new NullPointerException();
    }
    
    encoding_ = value.getNumber();
  }
  /**
   * <pre>
   * *Required* Encoding of audio data sent in all `RecognitionAudio` messages.
   * </pre>
   *
   * <code>optional .google.cloud.speech.v1.RecognitionConfig.AudioEncoding encoding = 1;</code>
   */
  private void clearEncoding() {
    
    encoding_ = 0;
  }

  public static final int SAMPLE_RATE_HERTZ_FIELD_NUMBER = 2;
  private int sampleRateHertz_;
  /**
   * <pre>
   * *Required* Sample rate in Hertz of the audio data sent in all
   * `RecognitionAudio` messages. Valid values are: 8000-48000.
   * 16000 is optimal. For best results, set the sampling rate of the audio
   * source to 16000 Hz. If that's not possible, use the native sample rate of
   * the audio source (instead of re-sampling).
   * </pre>
   *
   * <code>optional int32 sample_rate_hertz = 2;</code>
   */
  public int getSampleRateHertz() {
    return sampleRateHertz_;
  }
  /**
   * <pre>
   * *Required* Sample rate in Hertz of the audio data sent in all
   * `RecognitionAudio` messages. Valid values are: 8000-48000.
   * 16000 is optimal. For best results, set the sampling rate of the audio
   * source to 16000 Hz. If that's not possible, use the native sample rate of
   * the audio source (instead of re-sampling).
   * </pre>
   *
   * <code>optional int32 sample_rate_hertz = 2;</code>
   */
  private void setSampleRateHertz(int value) {
    
    sampleRateHertz_ = value;
  }
  /**
   * <pre>
   * *Required* Sample rate in Hertz of the audio data sent in all
   * `RecognitionAudio` messages. Valid values are: 8000-48000.
   * 16000 is optimal. For best results, set the sampling rate of the audio
   * source to 16000 Hz. If that's not possible, use the native sample rate of
   * the audio source (instead of re-sampling).
   * </pre>
   *
   * <code>optional int32 sample_rate_hertz = 2;</code>
   */
  private void clearSampleRateHertz() {
    
    sampleRateHertz_ = 0;
  }

  public static final int LANGUAGE_CODE_FIELD_NUMBER = 3;
  private java.lang.String languageCode_;
  /**
   * <pre>
   * *Required* The language of the supplied audio as a
   * [BCP-47](https://www.rfc-editor.org/rfc/bcp/bcp47.txt) language tag.
   * Example: "en-US".
   * See [Language Support](https://cloud.google.com/speech/docs/languages)
   * for a list of the currently supported language codes.
   * </pre>
   *
   * <code>optional string language_code = 3;</code>
   */
  public java.lang.String getLanguageCode() {
    return languageCode_;
  }
  /**
   * <pre>
   * *Required* The language of the supplied audio as a
   * [BCP-47](https://www.rfc-editor.org/rfc/bcp/bcp47.txt) language tag.
   * Example: "en-US".
   * See [Language Support](https://cloud.google.com/speech/docs/languages)
   * for a list of the currently supported language codes.
   * </pre>
   *
   * <code>optional string language_code = 3;</code>
   */
  public com.google.protobuf.ByteString
      getLanguageCodeBytes() {
    return com.google.protobuf.ByteString.copyFromUtf8(languageCode_);
  }
  /**
   * <pre>
   * *Required* The language of the supplied audio as a
   * [BCP-47](https://www.rfc-editor.org/rfc/bcp/bcp47.txt) language tag.
   * Example: "en-US".
   * See [Language Support](https://cloud.google.com/speech/docs/languages)
   * for a list of the currently supported language codes.
   * </pre>
   *
   * <code>optional string language_code = 3;</code>
   */
  private void setLanguageCode(
      java.lang.String value) {
    if (value == null) {
    throw new NullPointerException();
  }
  
    languageCode_ = value;
  }
  /**
   * <pre>
   * *Required* The language of the supplied audio as a
   * [BCP-47](https://www.rfc-editor.org/rfc/bcp/bcp47.txt) language tag.
   * Example: "en-US".
   * See [Language Support](https://cloud.google.com/speech/docs/languages)
   * for a list of the currently supported language codes.
   * </pre>
   *
   * <code>optional string language_code = 3;</code>
   */
  private void clearLanguageCode() {
    
    languageCode_ = getDefaultInstance().getLanguageCode();
  }
  /**
   * <pre>
   * *Required* The language of the supplied audio as a
   * [BCP-47](https://www.rfc-editor.org/rfc/bcp/bcp47.txt) language tag.
   * Example: "en-US".
   * See [Language Support](https://cloud.google.com/speech/docs/languages)
   * for a list of the currently supported language codes.
   * </pre>
   *
   * <code>optional string language_code = 3;</code>
   */
  private void setLanguageCodeBytes(
      com.google.protobuf.ByteString value) {
    if (value == null) {
    throw new NullPointerException();
  }
  checkByteStringIsUtf8(value);
    
    languageCode_ = value.toStringUtf8();
  }

  public static final int MAX_ALTERNATIVES_FIELD_NUMBER = 4;
  private int maxAlternatives_;
  /**
   * <pre>
   * *Optional* Maximum number of recognition hypotheses to be returned.
   * Specifically, the maximum number of `SpeechRecognitionAlternative` messages
   * within each `SpeechRecognitionResult`.
   * The server may return fewer than `max_alternatives`.
   * Valid values are `0`-`30`. A value of `0` or `1` will return a maximum of
   * one. If omitted, will return a maximum of one.
   * </pre>
   *
   * <code>optional int32 max_alternatives = 4;</code>
   */
  public int getMaxAlternatives() {
    return maxAlternatives_;
  }
  /**
   * <pre>
   * *Optional* Maximum number of recognition hypotheses to be returned.
   * Specifically, the maximum number of `SpeechRecognitionAlternative` messages
   * within each `SpeechRecognitionResult`.
   * The server may return fewer than `max_alternatives`.
   * Valid values are `0`-`30`. A value of `0` or `1` will return a maximum of
   * one. If omitted, will return a maximum of one.
   * </pre>
   *
   * <code>optional int32 max_alternatives = 4;</code>
   */
  private void setMaxAlternatives(int value) {
    
    maxAlternatives_ = value;
  }
  /**
   * <pre>
   * *Optional* Maximum number of recognition hypotheses to be returned.
   * Specifically, the maximum number of `SpeechRecognitionAlternative` messages
   * within each `SpeechRecognitionResult`.
   * The server may return fewer than `max_alternatives`.
   * Valid values are `0`-`30`. A value of `0` or `1` will return a maximum of
   * one. If omitted, will return a maximum of one.
   * </pre>
   *
   * <code>optional int32 max_alternatives = 4;</code>
   */
  private void clearMaxAlternatives() {
    
    maxAlternatives_ = 0;
  }

  public static final int PROFANITY_FILTER_FIELD_NUMBER = 5;
  private boolean profanityFilter_;
  /**
   * <pre>
   * *Optional* If set to `true`, the server will attempt to filter out
   * profanities, replacing all but the initial character in each filtered word
   * with asterisks, e.g. "f***". If set to `false` or omitted, profanities
   * won't be filtered out.
   * </pre>
   *
   * <code>optional bool profanity_filter = 5;</code>
   */
  public boolean getProfanityFilter() {
    return profanityFilter_;
  }
  /**
   * <pre>
   * *Optional* If set to `true`, the server will attempt to filter out
   * profanities, replacing all but the initial character in each filtered word
   * with asterisks, e.g. "f***". If set to `false` or omitted, profanities
   * won't be filtered out.
   * </pre>
   *
   * <code>optional bool profanity_filter = 5;</code>
   */
  private void setProfanityFilter(boolean value) {
    
    profanityFilter_ = value;
  }
  /**
   * <pre>
   * *Optional* If set to `true`, the server will attempt to filter out
   * profanities, replacing all but the initial character in each filtered word
   * with asterisks, e.g. "f***". If set to `false` or omitted, profanities
   * won't be filtered out.
   * </pre>
   *
   * <code>optional bool profanity_filter = 5;</code>
   */
  private void clearProfanityFilter() {
    
    profanityFilter_ = false;
  }

  public static final int SPEECH_CONTEXTS_FIELD_NUMBER = 6;
  private com.google.protobuf.Internal.ProtobufList<com.google.cloud.speech.v1.SpeechContext> speechContexts_;
  /**
   * <pre>
   * *Optional* A means to provide context to assist the speech recognition.
   * </pre>
   *
   * <code>repeated .google.cloud.speech.v1.SpeechContext speech_contexts = 6;</code>
   */
  public java.util.List<com.google.cloud.speech.v1.SpeechContext> getSpeechContextsList() {
    return speechContexts_;
  }
  /**
   * <pre>
   * *Optional* A means to provide context to assist the speech recognition.
   * </pre>
   *
   * <code>repeated .google.cloud.speech.v1.SpeechContext speech_contexts = 6;</code>
   */
  public java.util.List<? extends com.google.cloud.speech.v1.SpeechContextOrBuilder> 
      getSpeechContextsOrBuilderList() {
    return speechContexts_;
  }
  /**
   * <pre>
   * *Optional* A means to provide context to assist the speech recognition.
   * </pre>
   *
   * <code>repeated .google.cloud.speech.v1.SpeechContext speech_contexts = 6;</code>
   */
  public int getSpeechContextsCount() {
    return speechContexts_.size();
  }
  /**
   * <pre>
   * *Optional* A means to provide context to assist the speech recognition.
   * </pre>
   *
   * <code>repeated .google.cloud.speech.v1.SpeechContext speech_contexts = 6;</code>
   */
  public com.google.cloud.speech.v1.SpeechContext getSpeechContexts(int index) {
    return speechContexts_.get(index);
  }
  /**
   * <pre>
   * *Optional* A means to provide context to assist the speech recognition.
   * </pre>
   *
   * <code>repeated .google.cloud.speech.v1.SpeechContext speech_contexts = 6;</code>
   */
  public com.google.cloud.speech.v1.SpeechContextOrBuilder getSpeechContextsOrBuilder(
      int index) {
    return speechContexts_.get(index);
  }
  private void ensureSpeechContextsIsMutable() {
    if (!speechContexts_.isModifiable()) {
      speechContexts_ =
          com.google.protobuf.GeneratedMessageLite.mutableCopy(speechContexts_);
     }
  }

  /**
   * <pre>
   * *Optional* A means to provide context to assist the speech recognition.
   * </pre>
   *
   * <code>repeated .google.cloud.speech.v1.SpeechContext speech_contexts = 6;</code>
   */
  private void setSpeechContexts(
      int index, com.google.cloud.speech.v1.SpeechContext value) {
    if (value == null) {
      throw new NullPointerException();
    }
    ensureSpeechContextsIsMutable();
    speechContexts_.set(index, value);
  }
  /**
   * <pre>
   * *Optional* A means to provide context to assist the speech recognition.
   * </pre>
   *
   * <code>repeated .google.cloud.speech.v1.SpeechContext speech_contexts = 6;</code>
   */
  private void setSpeechContexts(
      int index, com.google.cloud.speech.v1.SpeechContext.Builder builderForValue) {
    ensureSpeechContextsIsMutable();
    speechContexts_.set(index, builderForValue.build());
  }
  /**
   * <pre>
   * *Optional* A means to provide context to assist the speech recognition.
   * </pre>
   *
   * <code>repeated .google.cloud.speech.v1.SpeechContext speech_contexts = 6;</code>
   */
  private void addSpeechContexts(com.google.cloud.speech.v1.SpeechContext value) {
    if (value == null) {
      throw new NullPointerException();
    }
    ensureSpeechContextsIsMutable();
    speechContexts_.add(value);
  }
  /**
   * <pre>
   * *Optional* A means to provide context to assist the speech recognition.
   * </pre>
   *
   * <code>repeated .google.cloud.speech.v1.SpeechContext speech_contexts = 6;</code>
   */
  private void addSpeechContexts(
      int index, com.google.cloud.speech.v1.SpeechContext value) {
    if (value == null) {
      throw new NullPointerException();
    }
    ensureSpeechContextsIsMutable();
    speechContexts_.add(index, value);
  }
  /**
   * <pre>
   * *Optional* A means to provide context to assist the speech recognition.
   * </pre>
   *
   * <code>repeated .google.cloud.speech.v1.SpeechContext speech_contexts = 6;</code>
   */
  private void addSpeechContexts(
      com.google.cloud.speech.v1.SpeechContext.Builder builderForValue) {
    ensureSpeechContextsIsMutable();
    speechContexts_.add(builderForValue.build());
  }
  /**
   * <pre>
   * *Optional* A means to provide context to assist the speech recognition.
   * </pre>
   *
   * <code>repeated .google.cloud.speech.v1.SpeechContext speech_contexts = 6;</code>
   */
  private void addSpeechContexts(
      int index, com.google.cloud.speech.v1.SpeechContext.Builder builderForValue) {
    ensureSpeechContextsIsMutable();
    speechContexts_.add(index, builderForValue.build());
  }
  /**
   * <pre>
   * *Optional* A means to provide context to assist the speech recognition.
   * </pre>
   *
   * <code>repeated .google.cloud.speech.v1.SpeechContext speech_contexts = 6;</code>
   */
  private void addAllSpeechContexts(
      java.lang.Iterable<? extends com.google.cloud.speech.v1.SpeechContext> values) {
    ensureSpeechContextsIsMutable();
    com.google.protobuf.AbstractMessageLite.addAll(
        values, speechContexts_);
  }
  /**
   * <pre>
   * *Optional* A means to provide context to assist the speech recognition.
   * </pre>
   *
   * <code>repeated .google.cloud.speech.v1.SpeechContext speech_contexts = 6;</code>
   */
  private void clearSpeechContexts() {
    speechContexts_ = emptyProtobufList();
  }
  /**
   * <pre>
   * *Optional* A means to provide context to assist the speech recognition.
   * </pre>
   *
   * <code>repeated .google.cloud.speech.v1.SpeechContext speech_contexts = 6;</code>
   */
  private void removeSpeechContexts(int index) {
    ensureSpeechContextsIsMutable();
    speechContexts_.remove(index);
  }

  public static final int ENABLE_WORD_TIME_OFFSETS_FIELD_NUMBER = 8;
  private boolean enableWordTimeOffsets_;
  /**
   * <pre>
   * *Optional* If `true`, the top result includes a list of words and
   * the start and end time offsets (timestamps) for those words. If
   * `false`, no word-level time offset information is returned. The default is
   * `false`.
   * </pre>
   *
   * <code>optional bool enable_word_time_offsets = 8;</code>
   */
  public boolean getEnableWordTimeOffsets() {
    return enableWordTimeOffsets_;
  }
  /**
   * <pre>
   * *Optional* If `true`, the top result includes a list of words and
   * the start and end time offsets (timestamps) for those words. If
   * `false`, no word-level time offset information is returned. The default is
   * `false`.
   * </pre>
   *
   * <code>optional bool enable_word_time_offsets = 8;</code>
   */
  private void setEnableWordTimeOffsets(boolean value) {
    
    enableWordTimeOffsets_ = value;
  }
  /**
   * <pre>
   * *Optional* If `true`, the top result includes a list of words and
   * the start and end time offsets (timestamps) for those words. If
   * `false`, no word-level time offset information is returned. The default is
   * `false`.
   * </pre>
   *
   * <code>optional bool enable_word_time_offsets = 8;</code>
   */
  private void clearEnableWordTimeOffsets() {
    
    enableWordTimeOffsets_ = false;
  }

  public void writeTo(com.google.protobuf.CodedOutputStream output)
                      throws java.io.IOException {
    if (encoding_ != com.google.cloud.speech.v1.RecognitionConfig.AudioEncoding.ENCODING_UNSPECIFIED.getNumber()) {
      output.writeEnum(1, encoding_);
    }
    if (sampleRateHertz_ != 0) {
      output.writeInt32(2, sampleRateHertz_);
    }
    if (!languageCode_.isEmpty()) {
      output.writeString(3, getLanguageCode());
    }
    if (maxAlternatives_ != 0) {
      output.writeInt32(4, maxAlternatives_);
    }
    if (profanityFilter_ != false) {
      output.writeBool(5, profanityFilter_);
    }
    for (int i = 0; i < speechContexts_.size(); i++) {
      output.writeMessage(6, speechContexts_.get(i));
    }
    if (enableWordTimeOffsets_ != false) {
      output.writeBool(8, enableWordTimeOffsets_);
    }
  }

  public int getSerializedSize() {
    int size = memoizedSerializedSize;
    if (size != -1) return size;

    size = 0;
    if (encoding_ != com.google.cloud.speech.v1.RecognitionConfig.AudioEncoding.ENCODING_UNSPECIFIED.getNumber()) {
      size += com.google.protobuf.CodedOutputStream
        .computeEnumSize(1, encoding_);
    }
    if (sampleRateHertz_ != 0) {
      size += com.google.protobuf.CodedOutputStream
        .computeInt32Size(2, sampleRateHertz_);
    }
    if (!languageCode_.isEmpty()) {
      size += com.google.protobuf.CodedOutputStream
        .computeStringSize(3, getLanguageCode());
    }
    if (maxAlternatives_ != 0) {
      size += com.google.protobuf.CodedOutputStream
        .computeInt32Size(4, maxAlternatives_);
    }
    if (profanityFilter_ != false) {
      size += com.google.protobuf.CodedOutputStream
        .computeBoolSize(5, profanityFilter_);
    }
    for (int i = 0; i < speechContexts_.size(); i++) {
      size += com.google.protobuf.CodedOutputStream
        .computeMessageSize(6, speechContexts_.get(i));
    }
    if (enableWordTimeOffsets_ != false) {
      size += com.google.protobuf.CodedOutputStream
        .computeBoolSize(8, enableWordTimeOffsets_);
    }
    memoizedSerializedSize = size;
    return size;
  }

  public static com.google.cloud.speech.v1.RecognitionConfig parseFrom(
      com.google.protobuf.ByteString data)
      throws com.google.protobuf.InvalidProtocolBufferException {
    return com.google.protobuf.GeneratedMessageLite.parseFrom(
        DEFAULT_INSTANCE, data);
  }
  public static com.google.cloud.speech.v1.RecognitionConfig parseFrom(
      com.google.protobuf.ByteString data,
      com.google.protobuf.ExtensionRegistryLite extensionRegistry)
      throws com.google.protobuf.InvalidProtocolBufferException {
    return com.google.protobuf.GeneratedMessageLite.parseFrom(
        DEFAULT_INSTANCE, data, extensionRegistry);
  }
  public static com.google.cloud.speech.v1.RecognitionConfig parseFrom(byte[] data)
      throws com.google.protobuf.InvalidProtocolBufferException {
    return com.google.protobuf.GeneratedMessageLite.parseFrom(
        DEFAULT_INSTANCE, data);
  }
  public static com.google.cloud.speech.v1.RecognitionConfig parseFrom(
      byte[] data,
      com.google.protobuf.ExtensionRegistryLite extensionRegistry)
      throws com.google.protobuf.InvalidProtocolBufferException {
    return com.google.protobuf.GeneratedMessageLite.parseFrom(
        DEFAULT_INSTANCE, data, extensionRegistry);
  }
  public static com.google.cloud.speech.v1.RecognitionConfig parseFrom(java.io.InputStream input)
      throws java.io.IOException {
    return com.google.protobuf.GeneratedMessageLite.parseFrom(
        DEFAULT_INSTANCE, input);
  }
  public static com.google.cloud.speech.v1.RecognitionConfig parseFrom(
      java.io.InputStream input,
      com.google.protobuf.ExtensionRegistryLite extensionRegistry)
      throws java.io.IOException {
    return com.google.protobuf.GeneratedMessageLite.parseFrom(
        DEFAULT_INSTANCE, input, extensionRegistry);
  }
  public static com.google.cloud.speech.v1.RecognitionConfig parseDelimitedFrom(java.io.InputStream input)
      throws java.io.IOException {
    return parseDelimitedFrom(DEFAULT_INSTANCE, input);
  }
  public static com.google.cloud.speech.v1.RecognitionConfig parseDelimitedFrom(
      java.io.InputStream input,
      com.google.protobuf.ExtensionRegistryLite extensionRegistry)
      throws java.io.IOException {
    return parseDelimitedFrom(DEFAULT_INSTANCE, input, extensionRegistry);
  }
  public static com.google.cloud.speech.v1.RecognitionConfig parseFrom(
      com.google.protobuf.CodedInputStream input)
      throws java.io.IOException {
    return com.google.protobuf.GeneratedMessageLite.parseFrom(
        DEFAULT_INSTANCE, input);
  }
  public static com.google.cloud.speech.v1.RecognitionConfig parseFrom(
      com.google.protobuf.CodedInputStream input,
      com.google.protobuf.ExtensionRegistryLite extensionRegistry)
      throws java.io.IOException {
    return com.google.protobuf.GeneratedMessageLite.parseFrom(
        DEFAULT_INSTANCE, input, extensionRegistry);
  }

  public static Builder newBuilder() {
    return DEFAULT_INSTANCE.toBuilder();
  }
  public static Builder newBuilder(com.google.cloud.speech.v1.RecognitionConfig prototype) {
    return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
  }

  /**
   * <pre>
   * Provides information to the recognizer that specifies how to process the
   * request.
   * </pre>
   *
   * Protobuf type {@code google.cloud.speech.v1.RecognitionConfig}
   */
  public static final class Builder extends
      com.google.protobuf.GeneratedMessageLite.Builder<
        com.google.cloud.speech.v1.RecognitionConfig, Builder> implements
      // @@protoc_insertion_point(builder_implements:google.cloud.speech.v1.RecognitionConfig)
      com.google.cloud.speech.v1.RecognitionConfigOrBuilder {
    // Construct using com.google.cloud.speech.v1.RecognitionConfig.newBuilder()
    private Builder() {
      super(DEFAULT_INSTANCE);
    }


    /**
     * <pre>
     * *Required* Encoding of audio data sent in all `RecognitionAudio` messages.
     * </pre>
     *
     * <code>optional .google.cloud.speech.v1.RecognitionConfig.AudioEncoding encoding = 1;</code>
     */
    public int getEncodingValue() {
      return instance.getEncodingValue();
    }
    /**
     * <pre>
     * *Required* Encoding of audio data sent in all `RecognitionAudio` messages.
     * </pre>
     *
     * <code>optional .google.cloud.speech.v1.RecognitionConfig.AudioEncoding encoding = 1;</code>
     */
    public Builder setEncodingValue(int value) {
      copyOnWrite();
      instance.setEncodingValue(value);
      return this;
    }
    /**
     * <pre>
     * *Required* Encoding of audio data sent in all `RecognitionAudio` messages.
     * </pre>
     *
     * <code>optional .google.cloud.speech.v1.RecognitionConfig.AudioEncoding encoding = 1;</code>
     */
    public com.google.cloud.speech.v1.RecognitionConfig.AudioEncoding getEncoding() {
      return instance.getEncoding();
    }
    /**
     * <pre>
     * *Required* Encoding of audio data sent in all `RecognitionAudio` messages.
     * </pre>
     *
     * <code>optional .google.cloud.speech.v1.RecognitionConfig.AudioEncoding encoding = 1;</code>
     */
    public Builder setEncoding(com.google.cloud.speech.v1.RecognitionConfig.AudioEncoding value) {
      copyOnWrite();
      instance.setEncoding(value);
      return this;
    }
    /**
     * <pre>
     * *Required* Encoding of audio data sent in all `RecognitionAudio` messages.
     * </pre>
     *
     * <code>optional .google.cloud.speech.v1.RecognitionConfig.AudioEncoding encoding = 1;</code>
     */
    public Builder clearEncoding() {
      copyOnWrite();
      instance.clearEncoding();
      return this;
    }

    /**
     * <pre>
     * *Required* Sample rate in Hertz of the audio data sent in all
     * `RecognitionAudio` messages. Valid values are: 8000-48000.
     * 16000 is optimal. For best results, set the sampling rate of the audio
     * source to 16000 Hz. If that's not possible, use the native sample rate of
     * the audio source (instead of re-sampling).
     * </pre>
     *
     * <code>optional int32 sample_rate_hertz = 2;</code>
     */
    public int getSampleRateHertz() {
      return instance.getSampleRateHertz();
    }
    /**
     * <pre>
     * *Required* Sample rate in Hertz of the audio data sent in all
     * `RecognitionAudio` messages. Valid values are: 8000-48000.
     * 16000 is optimal. For best results, set the sampling rate of the audio
     * source to 16000 Hz. If that's not possible, use the native sample rate of
     * the audio source (instead of re-sampling).
     * </pre>
     *
     * <code>optional int32 sample_rate_hertz = 2;</code>
     */
    public Builder setSampleRateHertz(int value) {
      copyOnWrite();
      instance.setSampleRateHertz(value);
      return this;
    }
    /**
     * <pre>
     * *Required* Sample rate in Hertz of the audio data sent in all
     * `RecognitionAudio` messages. Valid values are: 8000-48000.
     * 16000 is optimal. For best results, set the sampling rate of the audio
     * source to 16000 Hz. If that's not possible, use the native sample rate of
     * the audio source (instead of re-sampling).
     * </pre>
     *
     * <code>optional int32 sample_rate_hertz = 2;</code>
     */
    public Builder clearSampleRateHertz() {
      copyOnWrite();
      instance.clearSampleRateHertz();
      return this;
    }

    /**
     * <pre>
     * *Required* The language of the supplied audio as a
     * [BCP-47](https://www.rfc-editor.org/rfc/bcp/bcp47.txt) language tag.
     * Example: "en-US".
     * See [Language Support](https://cloud.google.com/speech/docs/languages)
     * for a list of the currently supported language codes.
     * </pre>
     *
     * <code>optional string language_code = 3;</code>
     */
    public java.lang.String getLanguageCode() {
      return instance.getLanguageCode();
    }
    /**
     * <pre>
     * *Required* The language of the supplied audio as a
     * [BCP-47](https://www.rfc-editor.org/rfc/bcp/bcp47.txt) language tag.
     * Example: "en-US".
     * See [Language Support](https://cloud.google.com/speech/docs/languages)
     * for a list of the currently supported language codes.
     * </pre>
     *
     * <code>optional string language_code = 3;</code>
     */
    public com.google.protobuf.ByteString
        getLanguageCodeBytes() {
      return instance.getLanguageCodeBytes();
    }
    /**
     * <pre>
     * *Required* The language of the supplied audio as a
     * [BCP-47](https://www.rfc-editor.org/rfc/bcp/bcp47.txt) language tag.
     * Example: "en-US".
     * See [Language Support](https://cloud.google.com/speech/docs/languages)
     * for a list of the currently supported language codes.
     * </pre>
     *
     * <code>optional string language_code = 3;</code>
     */
    public Builder setLanguageCode(
        java.lang.String value) {
      copyOnWrite();
      instance.setLanguageCode(value);
      return this;
    }
    /**
     * <pre>
     * *Required* The language of the supplied audio as a
     * [BCP-47](https://www.rfc-editor.org/rfc/bcp/bcp47.txt) language tag.
     * Example: "en-US".
     * See [Language Support](https://cloud.google.com/speech/docs/languages)
     * for a list of the currently supported language codes.
     * </pre>
     *
     * <code>optional string language_code = 3;</code>
     */
    public Builder clearLanguageCode() {
      copyOnWrite();
      instance.clearLanguageCode();
      return this;
    }
    /**
     * <pre>
     * *Required* The language of the supplied audio as a
     * [BCP-47](https://www.rfc-editor.org/rfc/bcp/bcp47.txt) language tag.
     * Example: "en-US".
     * See [Language Support](https://cloud.google.com/speech/docs/languages)
     * for a list of the currently supported language codes.
     * </pre>
     *
     * <code>optional string language_code = 3;</code>
     */
    public Builder setLanguageCodeBytes(
        com.google.protobuf.ByteString value) {
      copyOnWrite();
      instance.setLanguageCodeBytes(value);
      return this;
    }

    /**
     * <pre>
     * *Optional* Maximum number of recognition hypotheses to be returned.
     * Specifically, the maximum number of `SpeechRecognitionAlternative` messages
     * within each `SpeechRecognitionResult`.
     * The server may return fewer than `max_alternatives`.
     * Valid values are `0`-`30`. A value of `0` or `1` will return a maximum of
     * one. If omitted, will return a maximum of one.
     * </pre>
     *
     * <code>optional int32 max_alternatives = 4;</code>
     */
    public int getMaxAlternatives() {
      return instance.getMaxAlternatives();
    }
    /**
     * <pre>
     * *Optional* Maximum number of recognition hypotheses to be returned.
     * Specifically, the maximum number of `SpeechRecognitionAlternative` messages
     * within each `SpeechRecognitionResult`.
     * The server may return fewer than `max_alternatives`.
     * Valid values are `0`-`30`. A value of `0` or `1` will return a maximum of
     * one. If omitted, will return a maximum of one.
     * </pre>
     *
     * <code>optional int32 max_alternatives = 4;</code>
     */
    public Builder setMaxAlternatives(int value) {
      copyOnWrite();
      instance.setMaxAlternatives(value);
      return this;
    }
    /**
     * <pre>
     * *Optional* Maximum number of recognition hypotheses to be returned.
     * Specifically, the maximum number of `SpeechRecognitionAlternative` messages
     * within each `SpeechRecognitionResult`.
     * The server may return fewer than `max_alternatives`.
     * Valid values are `0`-`30`. A value of `0` or `1` will return a maximum of
     * one. If omitted, will return a maximum of one.
     * </pre>
     *
     * <code>optional int32 max_alternatives = 4;</code>
     */
    public Builder clearMaxAlternatives() {
      copyOnWrite();
      instance.clearMaxAlternatives();
      return this;
    }

    /**
     * <pre>
     * *Optional* If set to `true`, the server will attempt to filter out
     * profanities, replacing all but the initial character in each filtered word
     * with asterisks, e.g. "f***". If set to `false` or omitted, profanities
     * won't be filtered out.
     * </pre>
     *
     * <code>optional bool profanity_filter = 5;</code>
     */
    public boolean getProfanityFilter() {
      return instance.getProfanityFilter();
    }
    /**
     * <pre>
     * *Optional* If set to `true`, the server will attempt to filter out
     * profanities, replacing all but the initial character in each filtered word
     * with asterisks, e.g. "f***". If set to `false` or omitted, profanities
     * won't be filtered out.
     * </pre>
     *
     * <code>optional bool profanity_filter = 5;</code>
     */
    public Builder setProfanityFilter(boolean value) {
      copyOnWrite();
      instance.setProfanityFilter(value);
      return this;
    }
    /**
     * <pre>
     * *Optional* If set to `true`, the server will attempt to filter out
     * profanities, replacing all but the initial character in each filtered word
     * with asterisks, e.g. "f***". If set to `false` or omitted, profanities
     * won't be filtered out.
     * </pre>
     *
     * <code>optional bool profanity_filter = 5;</code>
     */
    public Builder clearProfanityFilter() {
      copyOnWrite();
      instance.clearProfanityFilter();
      return this;
    }

    /**
     * <pre>
     * *Optional* A means to provide context to assist the speech recognition.
     * </pre>
     *
     * <code>repeated .google.cloud.speech.v1.SpeechContext speech_contexts = 6;</code>
     */
    public java.util.List<com.google.cloud.speech.v1.SpeechContext> getSpeechContextsList() {
      return java.util.Collections.unmodifiableList(
          instance.getSpeechContextsList());
    }
    /**
     * <pre>
     * *Optional* A means to provide context to assist the speech recognition.
     * </pre>
     *
     * <code>repeated .google.cloud.speech.v1.SpeechContext speech_contexts = 6;</code>
     */
    public int getSpeechContextsCount() {
      return instance.getSpeechContextsCount();
    }/**
     * <pre>
     * *Optional* A means to provide context to assist the speech recognition.
     * </pre>
     *
     * <code>repeated .google.cloud.speech.v1.SpeechContext speech_contexts = 6;</code>
     */
    public com.google.cloud.speech.v1.SpeechContext getSpeechContexts(int index) {
      return instance.getSpeechContexts(index);
    }
    /**
     * <pre>
     * *Optional* A means to provide context to assist the speech recognition.
     * </pre>
     *
     * <code>repeated .google.cloud.speech.v1.SpeechContext speech_contexts = 6;</code>
     */
    public Builder setSpeechContexts(
        int index, com.google.cloud.speech.v1.SpeechContext value) {
      copyOnWrite();
      instance.setSpeechContexts(index, value);
      return this;
    }
    /**
     * <pre>
     * *Optional* A means to provide context to assist the speech recognition.
     * </pre>
     *
     * <code>repeated .google.cloud.speech.v1.SpeechContext speech_contexts = 6;</code>
     */
    public Builder setSpeechContexts(
        int index, com.google.cloud.speech.v1.SpeechContext.Builder builderForValue) {
      copyOnWrite();
      instance.setSpeechContexts(index, builderForValue);
      return this;
    }
    /**
     * <pre>
     * *Optional* A means to provide context to assist the speech recognition.
     * </pre>
     *
     * <code>repeated .google.cloud.speech.v1.SpeechContext speech_contexts = 6;</code>
     */
    public Builder addSpeechContexts(com.google.cloud.speech.v1.SpeechContext value) {
      copyOnWrite();
      instance.addSpeechContexts(value);
      return this;
    }
    /**
     * <pre>
     * *Optional* A means to provide context to assist the speech recognition.
     * </pre>
     *
     * <code>repeated .google.cloud.speech.v1.SpeechContext speech_contexts = 6;</code>
     */
    public Builder addSpeechContexts(
        int index, com.google.cloud.speech.v1.SpeechContext value) {
      copyOnWrite();
      instance.addSpeechContexts(index, value);
      return this;
    }
    /**
     * <pre>
     * *Optional* A means to provide context to assist the speech recognition.
     * </pre>
     *
     * <code>repeated .google.cloud.speech.v1.SpeechContext speech_contexts = 6;</code>
     */
    public Builder addSpeechContexts(
        com.google.cloud.speech.v1.SpeechContext.Builder builderForValue) {
      copyOnWrite();
      instance.addSpeechContexts(builderForValue);
      return this;
    }
    /**
     * <pre>
     * *Optional* A means to provide context to assist the speech recognition.
     * </pre>
     *
     * <code>repeated .google.cloud.speech.v1.SpeechContext speech_contexts = 6;</code>
     */
    public Builder addSpeechContexts(
        int index, com.google.cloud.speech.v1.SpeechContext.Builder builderForValue) {
      copyOnWrite();
      instance.addSpeechContexts(index, builderForValue);
      return this;
    }
    /**
     * <pre>
     * *Optional* A means to provide context to assist the speech recognition.
     * </pre>
     *
     * <code>repeated .google.cloud.speech.v1.SpeechContext speech_contexts = 6;</code>
     */
    public Builder addAllSpeechContexts(
        java.lang.Iterable<? extends com.google.cloud.speech.v1.SpeechContext> values) {
      copyOnWrite();
      instance.addAllSpeechContexts(values);
      return this;
    }
    /**
     * <pre>
     * *Optional* A means to provide context to assist the speech recognition.
     * </pre>
     *
     * <code>repeated .google.cloud.speech.v1.SpeechContext speech_contexts = 6;</code>
     */
    public Builder clearSpeechContexts() {
      copyOnWrite();
      instance.clearSpeechContexts();
      return this;
    }
    /**
     * <pre>
     * *Optional* A means to provide context to assist the speech recognition.
     * </pre>
     *
     * <code>repeated .google.cloud.speech.v1.SpeechContext speech_contexts = 6;</code>
     */
    public Builder removeSpeechContexts(int index) {
      copyOnWrite();
      instance.removeSpeechContexts(index);
      return this;
    }

    /**
     * <pre>
     * *Optional* If `true`, the top result includes a list of words and
     * the start and end time offsets (timestamps) for those words. If
     * `false`, no word-level time offset information is returned. The default is
     * `false`.
     * </pre>
     *
     * <code>optional bool enable_word_time_offsets = 8;</code>
     */
    public boolean getEnableWordTimeOffsets() {
      return instance.getEnableWordTimeOffsets();
    }
    /**
     * <pre>
     * *Optional* If `true`, the top result includes a list of words and
     * the start and end time offsets (timestamps) for those words. If
     * `false`, no word-level time offset information is returned. The default is
     * `false`.
     * </pre>
     *
     * <code>optional bool enable_word_time_offsets = 8;</code>
     */
    public Builder setEnableWordTimeOffsets(boolean value) {
      copyOnWrite();
      instance.setEnableWordTimeOffsets(value);
      return this;
    }
    /**
     * <pre>
     * *Optional* If `true`, the top result includes a list of words and
     * the start and end time offsets (timestamps) for those words. If
     * `false`, no word-level time offset information is returned. The default is
     * `false`.
     * </pre>
     *
     * <code>optional bool enable_word_time_offsets = 8;</code>
     */
    public Builder clearEnableWordTimeOffsets() {
      copyOnWrite();
      instance.clearEnableWordTimeOffsets();
      return this;
    }

    // @@protoc_insertion_point(builder_scope:google.cloud.speech.v1.RecognitionConfig)
  }
  protected final Object dynamicMethod(
      com.google.protobuf.GeneratedMessageLite.MethodToInvoke method,
      Object arg0, Object arg1) {
    switch (method) {
      case NEW_MUTABLE_INSTANCE: {
        return new com.google.cloud.speech.v1.RecognitionConfig();
      }
      case IS_INITIALIZED: {
        return DEFAULT_INSTANCE;
      }
      case MAKE_IMMUTABLE: {
        speechContexts_.makeImmutable();
        return null;
      }
      case NEW_BUILDER: {
        return new Builder();
      }
      case VISIT: {
        Visitor visitor = (Visitor) arg0;
        com.google.cloud.speech.v1.RecognitionConfig other = (com.google.cloud.speech.v1.RecognitionConfig) arg1;
        encoding_ = visitor.visitInt(encoding_ != 0, encoding_,    other.encoding_ != 0, other.encoding_);
        sampleRateHertz_ = visitor.visitInt(sampleRateHertz_ != 0, sampleRateHertz_,
            other.sampleRateHertz_ != 0, other.sampleRateHertz_);
        languageCode_ = visitor.visitString(!languageCode_.isEmpty(), languageCode_,
            !other.languageCode_.isEmpty(), other.languageCode_);
        maxAlternatives_ = visitor.visitInt(maxAlternatives_ != 0, maxAlternatives_,
            other.maxAlternatives_ != 0, other.maxAlternatives_);
        profanityFilter_ = visitor.visitBoolean(profanityFilter_ != false, profanityFilter_,
            other.profanityFilter_ != false, other.profanityFilter_);
        speechContexts_= visitor.visitList(speechContexts_, other.speechContexts_);
        enableWordTimeOffsets_ = visitor.visitBoolean(enableWordTimeOffsets_ != false, enableWordTimeOffsets_,
            other.enableWordTimeOffsets_ != false, other.enableWordTimeOffsets_);
        if (visitor == com.google.protobuf.GeneratedMessageLite.MergeFromVisitor
            .INSTANCE) {
          bitField0_ |= other.bitField0_;
        }
        return this;
      }
      case MERGE_FROM_STREAM: {
        com.google.protobuf.CodedInputStream input =
            (com.google.protobuf.CodedInputStream) arg0;
        com.google.protobuf.ExtensionRegistryLite extensionRegistry =
            (com.google.protobuf.ExtensionRegistryLite) arg1;
        try {
          boolean done = false;
          while (!done) {
            int tag = input.readTag();
            switch (tag) {
              case 0:
                done = true;
                break;
              default: {
                if (!input.skipField(tag)) {
                  done = true;
                }
                break;
              }
              case 8: {
                int rawValue = input.readEnum();

                encoding_ = rawValue;
                break;
              }
              case 16: {

                sampleRateHertz_ = input.readInt32();
                break;
              }
              case 26: {
                String s = input.readStringRequireUtf8();

                languageCode_ = s;
                break;
              }
              case 32: {

                maxAlternatives_ = input.readInt32();
                break;
              }
              case 40: {

                profanityFilter_ = input.readBool();
                break;
              }
              case 50: {
                if (!speechContexts_.isModifiable()) {
                  speechContexts_ =
                      com.google.protobuf.GeneratedMessageLite.mutableCopy(speechContexts_);
                }
                speechContexts_.add(
                    input.readMessage(com.google.cloud.speech.v1.SpeechContext.parser(), extensionRegistry));
                break;
              }
              case 64: {

                enableWordTimeOffsets_ = input.readBool();
                break;
              }
            }
          }
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          throw new RuntimeException(e.setUnfinishedMessage(this));
        } catch (java.io.IOException e) {
          throw new RuntimeException(
              new com.google.protobuf.InvalidProtocolBufferException(
                  e.getMessage()).setUnfinishedMessage(this));
        } finally {
        }
      }
      case GET_DEFAULT_INSTANCE: {
        return DEFAULT_INSTANCE;
      }
      case GET_PARSER: {
        if (PARSER == null) {    synchronized (com.google.cloud.speech.v1.RecognitionConfig.class) {
            if (PARSER == null) {
              PARSER = new DefaultInstanceBasedParser(DEFAULT_INSTANCE);
            }
          }
        }
        return PARSER;
      }
    }
    throw new UnsupportedOperationException();
  }


  // @@protoc_insertion_point(class_scope:google.cloud.speech.v1.RecognitionConfig)
  private static final com.google.cloud.speech.v1.RecognitionConfig DEFAULT_INSTANCE;
  static {
    DEFAULT_INSTANCE = new RecognitionConfig();
    DEFAULT_INSTANCE.makeImmutable();
  }

  public static com.google.cloud.speech.v1.RecognitionConfig getDefaultInstance() {
    return DEFAULT_INSTANCE;
  }

  private static volatile com.google.protobuf.Parser<RecognitionConfig> PARSER;

  public static com.google.protobuf.Parser<RecognitionConfig> parser() {
    return DEFAULT_INSTANCE.getParserForType();
  }
}

